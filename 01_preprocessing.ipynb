{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import statistics\n",
    "from collections import Counter\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data/02_postprocessing direction if it does not exist\n",
    "if not os.path.exists(\"data/02_postprocessing\"):\n",
    "    os.makedirs(\"data/02_postprocessing\")\n",
    "# Create the data/02_postprocessing/sample directory if it does not exist\n",
    "if not os.path.exists(\"data/02_postprocessing/sample\"):\n",
    "    os.makedirs(\"data/02_postprocessing/sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [!] 4 minutes to load\n",
    "with open(f\"data/01_cleaned/{'sample/' if SAMPLE else ''}reviews.json\") as f:\n",
    "    reviews_original = [json.loads(line) for line in f]\n",
    "with open(f\"data/01_cleaned/{'sample/' if SAMPLE else ''}users.json\") as f:\n",
    "    users_original = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews_dict = {}\n",
    "for review in reviews_original:\n",
    "    user_id = review[\"user_id\"]\n",
    "    if user_id not in user_reviews_dict.keys():\n",
    "        user_reviews_dict[user_id] = {\"reviews\": [review], \"ratings\": [(review[\"review_id\"], review[\"stars\"])]}\n",
    "    else:\n",
    "        user_reviews_dict[user_id][\"reviews\"].append(review)\n",
    "        user_reviews_dict[user_id][\"ratings\"].append((review[\"review_id\"], review[\"stars\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to determine the cut off for how many reviews a user should have before we\n",
    "# apply z-score normalization\n",
    "\n",
    "# This outputs the ratio between the number of reviews that would be standardized\n",
    "# and all reviews in the dataset\n",
    "\n",
    "cutoff = 5\n",
    "num_reviews = [len(user[\"reviews\"]) for user in user_reviews_dict.values()]\n",
    "review_count = Counter(num_reviews)\n",
    "review_counter_agg = [(tup[0], tup[1], tup[0] * tup[1]) for tup in review_count.items()]\n",
    "ratio = sum([tup[2] for tup in review_counter_agg if tup[0] >= cutoff]) / sum([tup[2] for tup in review_counter_agg])\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_not_standardized = []\n",
    "users_standardized = []\n",
    "for user_id, user_reviews in user_reviews_dict.items():\n",
    "    if len(user_reviews[\"ratings\"]) >= cutoff and len(set(list(zip(*user_reviews[\"ratings\"]))[1])) >= 2:\n",
    "        users_standardized.append(user_id)\n",
    "        user_reviews[\"avg_rating\"] = statistics.mean(list(zip(*user_reviews[\"ratings\"]))[1])\n",
    "        user_reviews[\"std_dev\"] = statistics.stdev(list(zip(*user_reviews[\"ratings\"]))[1])\n",
    "        user_reviews[\"standardized_ratings\"] = [(rating[0], (rating[1] - user_reviews[\"avg_rating\"]) / user_reviews[\"std_dev\"]) for rating in user_reviews[\"ratings\"]]\n",
    "    else:\n",
    "        users_not_standardized.append(user_id)\n",
    "        \n",
    "# standardize all other ratings with respect to each other\n",
    "all_ratings = [rating for user_reviews in [user_reviews_dict[user_id] for user_id in users_not_standardized] for rating in user_reviews[\"ratings\"]]\n",
    "mean = statistics.mean(list(zip(*all_ratings))[1])\n",
    "std_dev = statistics.stdev(list(zip(*all_ratings))[1])\n",
    "pool_standardized_ratings = [(rating[0], (rating[1] - mean) / std_dev) for rating in all_ratings]\n",
    "\n",
    "# Gather all ratings standardized by user\n",
    "user_standardized_ratings = [rating for user_reviews in [user_review for user_review in user_reviews_dict.values() if \"standardized_ratings\" in user_review.keys()] for rating in user_reviews[\"standardized_ratings\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of users: {len(user_reviews_dict)}\")\n",
    "print(f\"Number of users with at least {cutoff} reviews: {len(users_standardized)}\")\n",
    "print(f\"Number of users with less than {cutoff} reviews: {len(users_not_standardized)}\")\n",
    "print(f\"Number of ratings that were user-standardized: {len(user_standardized_ratings)}\")\n",
    "print(f\"Number of ratings that were pool-standardized: {len(pool_standardized_ratings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {review[\"review_id\"]: review for review in reviews_original}\n",
    "for rating in user_standardized_ratings + pool_standardized_ratings:\n",
    "    reviews[rating[0]][\"standardized_rating\"] = rating[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"data/02_postprocessing/{'sample/' if SAMPLE else ''}reviews.json\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "with open(f\"data/02_postprocessing/{'sample/' if SAMPLE else ''}reviews.json\", \"a\") as f:\n",
    "    for review in reviews.values():\n",
    "        f.write(json.dumps(review) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# read business JSON file\n",
    "with open(f\"data/01_cleaned/{'sample/' if SAMPLE else ''}businesses.json\") as f:\n",
    "    businesses = [json.loads(line) for line in f]\n",
    "\n",
    "# Extract keywords from each business entry for category and name attributes using dictionary\n",
    "business_keywords = {}\n",
    "for business in businesses:\n",
    "    business_keywords[business[\"business_id\"]] = {}\n",
    "    business_keywords[business[\"business_id\"]][\"category\"] = []\n",
    "    business_keywords[business[\"business_id\"]][\"name\"] = []\n",
    "    for word in word_tokenize(business[\"categories\"]):\n",
    "        if word not in stop_words and word != \",\" and word != \"&\" and word != \"-\" and word != \"(\" and word != \")\" and word != \".\" and word != \"'\" and word != \"!\" and word != \"?\" and word != \":\" and word != \";\" and word != \"[\" and word != \"]\" and word != \"/\":\n",
    "            business_keywords[business[\"business_id\"]][\"category\"].append(word)\n",
    "    for word in word_tokenize(business[\"name\"]):\n",
    "        if word not in stop_words and word != \",\" and word != \"&\" and word != \"-\" and word != \"(\" and word != \")\" and word != \".\" and word != \"'\" and word != \"!\" and word != \"?\" and word != \":\" and word != \";\" and word != \"[\" and word != \"]\" and word != \"/\":\n",
    "            business_keywords[business[\"business_id\"]][\"name\"].append(word)\n",
    "\n",
    "# read review JSON file\n",
    "with open(f\"data/02_postprocessing/{'sample/' if SAMPLE else ''}reviews.json\") as f:\n",
    "    reviews = [json.loads(line) for line in f]\n",
    "\n",
    "# Extract keywords from each review entry for text attribute using dictionary\n",
    "review_keywords = {}\n",
    "for review in reviews:\n",
    "    review_keywords[review[\"review_id\"]] = []\n",
    "    for word in word_tokenize(review[\"text\"]):\n",
    "        if word not in stop_words and word != \",\" and word != \"&\" and word != \"-\" and word != \"(\" and word != \")\" and word != \".\" and word != \"'\" and word != \"!\" and word != \"?\" and word != \":\" and word != \";\" and word != \"[\" and word != \"]\" and word != \"/\":\n",
    "            review_keywords[review[\"review_id\"]].append(word)\n",
    "\n",
    "# Create a dictionary of business_id to a list of review_ids\n",
    "business_review_dict = {}\n",
    "for review in reviews:\n",
    "    if review[\"business_id\"] not in business_review_dict.keys():\n",
    "        business_review_dict[review[\"business_id\"]] = [review[\"review_id\"]]\n",
    "    else:\n",
    "        business_review_dict[review[\"business_id\"]].append(review[\"review_id\"])\n",
    "\n",
    "# combine keywords from all reviews, category, and name for each business\n",
    "# some businesses do not have any reviews, so we only combine category and name keywords for those businesses using dictionary\n",
    "business_keywords_combined = {}\n",
    "for business in businesses:\n",
    "    business_keywords_combined[business[\"business_id\"]] = []\n",
    "    if business[\"business_id\"] in business_review_dict.keys():\n",
    "        for review_id in business_review_dict[business[\"business_id\"]]:\n",
    "            business_keywords_combined[business[\"business_id\"]] += review_keywords[review_id]\n",
    "    business_keywords_combined[business[\"business_id\"]] += business_keywords[business[\"business_id\"]][\"category\"]\n",
    "    business_keywords_combined[business[\"business_id\"]] += business_keywords[business[\"business_id\"]][\"name\"]\n",
    "\n",
    "\n",
    "\n",
    "# print the keywords for each business with their name\n",
    "for business in businesses:\n",
    "    print(business[\"name\"] + \": \")\n",
    "    print(business_keywords_combined[business[\"business_id\"]])\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
